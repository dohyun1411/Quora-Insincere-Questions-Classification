{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prj_quora",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdOiqkAj7HZOG1BFrTek2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dohyun1411/Quora-Insincere-Questions-Classification/blob/main/prj_quora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crtt6CIMRVRX"
      },
      "source": [
        "###**Mount your Google drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AlF5r7k_l7B",
        "outputId": "7259cd95-fd8b-4c80-b700-a16bbd8a1d6f"
      },
      "source": [
        "from google.colab import drive\n",
        "import os, re, io, gc\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "root = '/gdrive/My Drive/Colab Notebooks/Project_Quora'\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/gdrive/My Drive/Colab Notebooks/Project_Quora\"\n",
        "%cd $root"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/Colab Notebooks/Project_Quora\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t17ylcWQpXMm"
      },
      "source": [
        "#download from kaggle and unzip\n",
        "def download_file():\n",
        "  !kaggle competitions download -c quora-insincere-questions-classification\n",
        "  !unzip \\*.zip\n",
        "#download_file()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfZNat7BRrdG"
      },
      "source": [
        "###**Install and Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqE3rLllrMLw",
        "outputId": "5ea9d3ec-a81f-404a-8dae-792a641fac63"
      },
      "source": [
        "from platform import python_version\n",
        "print('python', python_version())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaqRHkFCvrDc"
      },
      "source": [
        "import operator\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Bidirectional, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, GlobalMaxPool1D, SpatialDropout1D, BatchNormalization\n",
        "from tensorflow.python.client import device_lib\n",
        "from wordcloud import WordCloud\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVuQtRcDlGVB",
        "outputId": "dc43175d-2188-4e31-bb13-0263718e22df"
      },
      "source": [
        "def get_GPU():  \n",
        "  local_device_protos = device_lib.list_local_devices()\n",
        "  return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "print(get_GPU())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "Num GPUs Available:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZvkrrPXpqmN"
      },
      "source": [
        "###**Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0JuATImf-q6",
        "outputId": "b10be6dd-22d8-426b-a029-ba3c6dcc8f1e"
      },
      "source": [
        "train = pd.read_csv(root + \"/train.csv\")\n",
        "test = pd.read_csv(root + \"/test.csv\")\n",
        "percentage_insincere = round((train[\"target\"].values == 1).sum() / (train.shape[0]) * 100, 2)\n",
        "print(\"Train shape: {} and Test shape: {}\".format(train.shape, test.shape))\n",
        "print(\"Percentage of insincere questions in the train dataset: {}% \".format(percentage_insincere))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (1306122, 3) and Test shape: (375806, 2)\n",
            "Percentage of insincere questions in the train dataset: 6.19% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfnwSGSSFqpa"
      },
      "source": [
        "#Reduce sample data to save time while keeping the same ratio\n",
        "def reduce_data(sample_size, train, test, percentage_insincere):\n",
        "    train_sample_insincere = train.loc[train[\"target\"] == 1].sample(int((sample_size / 100) * percentage_insincere))\n",
        "    train_sample_sincere = train.loc[train[\"target\"] == 0].sample(int((sample_size / 100) * (100 - percentage_insincere)))\n",
        "    train = pd.concat([train_sample_insincere, train_sample_sincere], ignore_index = True)\n",
        "    train = shuffle(train)\n",
        "    test = test.sample(int(sample_size / 4))\n",
        "    percentage_insincere = round((train[\"target\"].values == 1).sum() / (train.shape[0]) * 100, 2)\n",
        "    print(\"Train shape: {} and Test shape: {}\".format(train.shape, test.shape))\n",
        "    print(\"Percentage of insincere questions in the train dataset: {}% \".format(percentage_insincere))\n",
        "    return train, test\n",
        "\n",
        "sample_size = 300000\n",
        "#train, test = reduce_data(sample_size, train, test, percentage_insincere)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkq4m2P_v9Su"
      },
      "source": [
        "contractions = {\n",
        "  \"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\", \"could've\": \"could have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"i'd\": \"I would\",\n",
        "  \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\", \"would've\": \"would have\",\n",
        "  \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n",
        "  \"mightn't\": \"might not\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"needn't\": \"need not\",\n",
        "  \"mustn't\": \"must not\", \"shan't\": \"shall not\", \"she'd\": \"she would\", \"she'll\": \"she will\",\n",
        "  \"she's\": \"she is\", \"shouldn't\": \"should not\", \"should've\": \"should have\", \"that's\": \"that is\",\n",
        "  \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\", \"weren't\": \"were not\",\n",
        "  \"we've\": \"we have\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\", \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n",
        "  \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\", \"who'll\": \"who will\",\n",
        "  \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\",\n",
        "  \"you're\": \"you are\", \"you've\": \"you have\", \"wasn't\": \"was not\", \"we'll\": \" will\",\n",
        "  \"didn't\": \"did not\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all're\": \"you all are\"\n",
        "}\n",
        "\n",
        "def data_cleaning(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '', text) # clean url\n",
        "  text = re.sub(r'#(\\w+)', '', text)   # clean hashtags\n",
        "  text = re.sub(r'@(\\w+)', '', text)   # clean @s\n",
        "  text = re.sub(r'<[^>]+>', '', text)  # clean tags\n",
        "  text = re.sub(r'\\d+', '', text)      # clean digits\n",
        "  text = re.sub(r'’', '\\'', text)      # replace ’ with '\n",
        "  text = re.sub(r's\\'', '', text)      # clean s'\n",
        "  text = re.sub(r'[£₹$€₩]', ' ', text) # clean currency symbols\n",
        "  text = re.sub(r'[δ∫βωδσ∈∆≡απθ+*-=°^×√÷]', ' ', text) # clean math symbols\n",
        "  text = re.sub(r'[/(),!@\"“”?.%_&#:;><{}~\\[\\]|…]', ' ', text)   # clean punctuation\n",
        "  text = [contractions[word] if word in contractions else word for word in text.split()]  # change contractions to full forms\n",
        "  text = [PorterStemmer().stem(word) for word in text] # stem\n",
        "  text = [WordNetLemmatizer().lemmatize(word) for word in text] # lemmatize\n",
        "  text = \" \".join(text)\n",
        "  text = re.sub(r'\\'s', '', text)      # clean 's\n",
        "  text = re.sub(r'\\'', '', text)       # clean '\n",
        "  return text\n",
        "\n",
        "def get_data_samples(data, target, x):\n",
        "  return data.loc[data[target] == x].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSS9Br4Y6PBh"
      },
      "source": [
        "#clean the questions\n",
        "train[\"cleaned_question_text\"] = train[\"question_text\"].apply(data_cleaning)\n",
        "test[\"cleaned_question_text\"] = test[\"question_text\"].apply(data_cleaning)\n",
        "total_sentences = pd.concat([train[\"cleaned_question_text\"], test[\"cleaned_question_text\"]], ignore_index = True)\n",
        "\n",
        "#get_data_samples(train,\"target\", 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erBnYIy-MH7i"
      },
      "source": [
        "def cloud(text, title, size = (10, 7)):\n",
        "  words_list = text.unique().tolist()\n",
        "  words = ' '.join(words_list)\n",
        "  wordcloud = WordCloud(width = 800, height = 400, collocations = False).generate(words)\n",
        "    \n",
        "  # Output Visualization\n",
        "  fig = plt.figure(figsize = size, dpi = 80, facecolor = \"k\", edgecolor = \"k\")\n",
        "  plt.imshow(wordcloud,interpolation = \"bilinear\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(title, fontsize=25,color = \"w\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "  plt.show()\n",
        "    \n",
        "#cloud(train[train['target'] == 0]['question_text_cleaned'], 'Cleaned Sincere questions')\n",
        "#cloud(train[train['target'] == 1]['question_text_cleaned'], 'Cleaned InSincere questions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlEL3_70MS1O"
      },
      "source": [
        "def vocabulary_builder(corpus):\n",
        "  vocabulary = {}\n",
        "  for text in corpus:\n",
        "    for word in text.split():\n",
        "      try:\n",
        "        vocabulary[word] += 1\n",
        "      except KeyError:\n",
        "        vocabulary[word] = 1\n",
        "  return vocabulary\n",
        "\n",
        "def get_word_index(vocabulary):\n",
        "  return dict((w, i + 1) for i, w in enumerate(vocabulary.keys()))\n",
        "  \n",
        "def visualize_sentence_length(my_list):\n",
        "  num_bins = 20\n",
        "  plt.hist(my_list, num_bins)\n",
        "  plt.show()\n",
        "\n",
        "sentence_length = [len(x) for x in total_sentences]\n",
        "vocabulary_set = vocabulary_builder(total_sentences)\n",
        "vocabulary_size = len(vocabulary_set) + 1\n",
        "word_index = get_word_index(vocabulary_set)\n",
        "#visualize_sentence_length(sentence_length)\n",
        "max_length = 55"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAByJTQyMQbl"
      },
      "source": [
        "embeddings = {\n",
        "            \"paragram\": \"./paragram_300_sl999/paragram_300_sl999.txt\",\n",
        "            \"glove\": \"./glove.840B.300d/glove.840B.300d.txt\",\n",
        "            \"wiki_news\": \"./wiki-news-300d-1M/wiki-news-300d-1M.vec\",\n",
        "            \"google_news\": \"./GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n",
        "            }\n",
        "\n",
        "def load_embeddings(file_name):\n",
        "  def get_coefs(word, *arr): \n",
        "    return word, np.asarray(arr, dtype = \"float32\")\n",
        "  print(\"Extracting \" + file_name + \" embedding\")\n",
        "\n",
        "  if file_name == list(embeddings.keys())[0]:\n",
        "    file = embeddings[\"paragram\"]\n",
        "    return dict(get_coefs(*o.split(\" \")) for o in open(file, encoding = \"utf8\", errors = \"ignore\") if len(o) > 100)\n",
        "  if file_name == list(embeddings.keys())[1]:\n",
        "    file = embeddings[\"glove\"]\n",
        "    return dict(get_coefs(*o.split(\" \")) for o in open(file))\n",
        "  if file_name == list(embeddings.keys())[2]:\n",
        "    file = embeddings[\"wiki_news\"]\n",
        "    return dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o) > 100)\n",
        "  if file_name == list(embeddings.keys())[3]:\n",
        "    file = embeddings[\"google_news\"]\n",
        "    return KeyedVectors.load_word2vec_format(file, binary = True)\n",
        "  else:\n",
        "    return \"Embedding File Doesn't Exist\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOUDhugSMf70"
      },
      "source": [
        "def load_embedding_matrices(embedding_choice):\n",
        "  count = 0\n",
        "  embedding_matrix = np.zeros((vocabulary_size, 300))\n",
        "  embedding_method = load_embeddings(embedding_choice)\n",
        "\n",
        "  def check_coverage(vocab, embeddings_index):\n",
        "    known_words = {}\n",
        "    unknown_words = {}\n",
        "    nb_known_words = 0\n",
        "    nb_unknown_words = 0\n",
        "    for word in vocab.keys():\n",
        "      try:\n",
        "        known_words[word] = embeddings_index[word]\n",
        "        nb_known_words += vocab[word]\n",
        "      except:\n",
        "        unknown_words[word] = vocab[word]\n",
        "        nb_unknown_words += vocab[word]\n",
        "        pass\n",
        "\n",
        "    print(\"Found embeddings for {:.2%} of vocabulary set\".format(len(known_words) / len(vocab)))\n",
        "    print(\"Found embeddings for {:.2%} of all text\".format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
        "    unknown_words = sorted(unknown_words.items(), key = operator.itemgetter(1))[::-1]\n",
        "    return unknown_words\n",
        "\n",
        "  def oov_check():\n",
        "    return check_coverage(vocabulary_set, embedding_method)\n",
        "\n",
        "  oov_check()\n",
        "\n",
        "  for word,i in word_index.items():\n",
        "    try:\n",
        "      vec = embedding_method[word]\n",
        "      embedding_matrix[i] = vec\n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      continue\n",
        "  del embedding_method\n",
        "  gc.collect()\n",
        "  return embedding_matrix, count\n",
        "\n",
        "embedding_choice_g = \"glove\"\n",
        "embedding_matrix_g, count_g = load_embedding_matrices(embedding_choice_g)\n",
        "print(\"Number of Out Of Vocabulary - OOVs in \" + embedding_choice_g + \": \", count_g)\n",
        "\n",
        "# embedding_choice_p = \"paragram\"\n",
        "# embedding_matrix_p, count_p = load_embedding_matrices(embedding_choice_p)\n",
        "# print(\"Number of Out Of Vocabulary - OOVs in \" + embedding_choice_p + \": \", count_p)\n",
        "# embedding_matrix = np.mean((1.2 * embedding_matrix_g, 0.8 * embedding_matrix_p), axis = 0)\n",
        "\n",
        "embedding_matrix = embedding_matrix_g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmY1sUUXMasY"
      },
      "source": [
        "train_set, val_set = train_test_split(train, test_size = 0.2, random_state = 11)\n",
        "train_set_Y, val_set_Y = np.array(train_set[\"target\"]), np.array(val_set[\"target\"])\n",
        "\n",
        "def use_encode():\n",
        "  def fit_one_hot(word_index, corpus):\n",
        "    sent = []\n",
        "    for text in corpus:\n",
        "      my_list = []\n",
        "      for word in text.split():\n",
        "        try:\n",
        "          my_list.append(word_index[word])\n",
        "        except KeyError:\n",
        "          my_list.append(0)\n",
        "      sent.append(my_list)\n",
        "    return sent\n",
        "  \n",
        "  training_sequence, validation_sequence, testing_sequence = train_set[\"cleaned_question_text\"], val_set[\"cleaned_question_text\"], test[\"cleaned_question_text\"]\n",
        "  encode_train_set, encode_val_set, encode_test_set = fit_one_hot(word_index, training_sequence), fit_one_hot(word_index, validation_sequence), fit_one_hot(word_index, testing_sequence)\n",
        "\n",
        "  x = pad_sequences(encode_train_set, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
        "  y = pad_sequences(encode_val_set, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
        "  z = pad_sequences(encode_test_set, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
        "  return x, y, z\n",
        "\n",
        "def use_tokenize():\n",
        "  tokenizer = Tokenizer(num_words = vocabulary_size, oov_token = \"<OOV>\")\n",
        "  tokenizer.fit_on_texts(total_sentences)\n",
        "\n",
        "  training_sequences = tokenizer.texts_to_sequences(train_set[\"cleaned_question_text\"])\n",
        "  validation_sequences = tokenizer.texts_to_sequences(val_set[\"cleaned_question_text\"])\n",
        "  testing_sequences = tokenizer.texts_to_sequences(test[\"cleaned_question_text\"])\n",
        "\n",
        "  x = pad_sequences(training_sequences, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
        "  y = pad_sequences(validation_sequences, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
        "  z = pad_sequences(testing_sequences, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
        "  return x, y, z\n",
        "\n",
        "training_padded, validation_padded, testing_padded = use_encode()\n",
        "#training_padded, validation_padded, testing_padded = use_tokenize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tRP07n6MjzB"
      },
      "source": [
        "BATCH_SIZE = 2048\n",
        "EPOCHS = 10\n",
        "inputs = Input(shape = (max_length,))\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor = \"loss\", patience = 2)\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.embed = Embedding(vocabulary_size, 300, weights = [embedding_matrix], trainable = False)\n",
        "    self.rnn = Sequential([\n",
        "                           SpatialDropout1D(0.3),\n",
        "                           Bidirectional(LSTM(128, return_sequences = True)),\n",
        "                           Bidirectional(LSTM(128, return_sequences = True)),\n",
        "                           Conv1D(128, 3, activation = \"relu\"),\n",
        "                           GlobalMaxPool1D(),\n",
        "                           Dense(64, activation = \"relu\"),\n",
        "                           Dropout(0.2),\n",
        "                           Dense(1, activation = \"sigmoid\")\n",
        "                          ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embed(inputs)\n",
        "    x = self.rnn(x)\n",
        "    return x\n",
        "\n",
        "model = MyModel()\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"Adam\", metrics = [\"binary_accuracy\"])\n",
        "model.fit(x = training_padded, y = train_set_Y, batch_size = BATCH_SIZE, epochs = EPOCHS, callbacks = callback, validation_data = (validation_padded, val_set_Y))\n",
        "predicted = model.predict(testing_padded, batch_size = 512)\n",
        "test['prediction'] = predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2JkqRd7nshl"
      },
      "source": [
        "test[\"#\"] = [0 if x < 0.5 else 1 for x in test[\"prediction\"]]\n",
        "percentage_insincere = round((test[\"#\"].values == 1).sum() / (test.shape[0]) * 100, 2)\n",
        "print(\"Percentage of insincere questions in the test dataset: {}% \".format(percentage_insincere))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXLK5UlHZxJK"
      },
      "source": [
        "#create csv for submission\n",
        "submission = pd.DataFrame({\"qid\": test[\"qid\"], \"question_text\": test[\"question_text\"], \"prediction\": test[\"#\"]})\n",
        "submission.to_csv(\"submission.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}