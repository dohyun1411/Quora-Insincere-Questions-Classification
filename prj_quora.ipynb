{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prj_quora",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNB3VsmxCJ6SwVFjE+xWxL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dohyun1411/Quora-Insincere-Questions-Classification/blob/main/prj_quora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crtt6CIMRVRX"
      },
      "source": [
        "###**Mount your Google drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AlF5r7k_l7B",
        "outputId": "226472ec-9502-43a4-cf1e-0a491e77449a"
      },
      "source": [
        "from google.colab import drive\n",
        "import os, re, io\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "root = '/gdrive/My Drive/Colab Notebooks/Project_Quora'\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/gdrive/My Drive/Colab Notebooks/Project_Quora\"\n",
        "%cd $root"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks/Project_Quora\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t17ylcWQpXMm"
      },
      "source": [
        "#download from kaggle and unzip\n",
        "def download_file():\n",
        "  !kaggle competitions download -c quora-insincere-questions-classification\n",
        "  !unzip \\*.zip\n",
        "#download_file()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfZNat7BRrdG"
      },
      "source": [
        "###**Install and Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqE3rLllrMLw",
        "outputId": "d72640f0-08b2-40a4-c4f4-7ba7b46302f6"
      },
      "source": [
        "from platform import python_version\n",
        "print('python', python_version())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaqRHkFCvrDc",
        "outputId": "6a95cce7-5d33-4b05-aaba-347ccbfea11a"
      },
      "source": [
        "import operator\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Bidirectional, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, GlobalMaxPool1D\n",
        "from tensorflow.python.client import device_lib\n",
        "from wordcloud import WordCloud\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVuQtRcDlGVB",
        "outputId": "31a13d42-46aa-4638-8f55-bc9a5fd9d74c"
      },
      "source": [
        "def get_GPU():  \n",
        "  local_device_protos = device_lib.list_local_devices()\n",
        "  return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "print(get_GPU())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/device:GPU:0']\n",
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZvkrrPXpqmN"
      },
      "source": [
        "###**Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0JuATImf-q6",
        "outputId": "a39c674b-c999-49b7-8b6b-7959be545a92"
      },
      "source": [
        "train = pd.read_csv(root + \"/train.csv\")\n",
        "test = pd.read_csv(root + \"/test.csv\")\n",
        "print(\"Train shape: {} and Test shape: {}\".format(train.shape, test.shape))\n",
        "percentage_insincere = round((train[\"target\"].values == 1).sum() / (train.shape[0]) * 100, 2)\n",
        "print(\"Percentage of insincere questions in the train dataset: {}% \".format(percentage_insincere))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (1306122, 3) and Test shape: (375806, 2)\n",
            "Percentage of insincere questions in the train dataset: 6.19% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfnwSGSSFqpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9289d6a-8e69-41d0-a8a9-591f7c73aa38"
      },
      "source": [
        "#Reduce sample data to save time while keeping the same ratio\n",
        "def reduce_data(sample_size, train, test):\n",
        "  train_sample_insincere = train.loc[train['target'] == 1].sample(int((sample_size / 100) * percentage_insincere))\n",
        "  train_sample_sincere = train.loc[train['target'] == 0].sample(int((sample_size / 100) * (100 - percentage_insincere)))\n",
        "  train = pd.concat([train_sample_insincere, train_sample_sincere], ignore_index = True)\n",
        "  train = shuffle(train)\n",
        "  test = test.sample(int(sample_size / 4))\n",
        "  return train, test\n",
        "\n",
        "sample_size = 300000\n",
        "#train, test = reduce_data(sample_size, train, test)\n",
        "print(\"Train shape: {} and Test shape: {}\".format(train.shape, test.shape))\n",
        "percentage_insincere = round((train[\"target\"].values == 1).sum() / (train.shape[0]) * 100, 2)\n",
        "print(\"Percentage of insincere questions in the train dataset: {}% \".format(percentage_insincere))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (1306122, 3) and Test shape: (375806, 2)\n",
            "Percentage of insincere questions in the train dataset: 6.19% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkq4m2P_v9Su"
      },
      "source": [
        "contractions = {\n",
        "  \"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\", \"could've\": \"could have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"i'd\": \"I would\",\n",
        "  \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\", \"would've\": \"would have\",\n",
        "  \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n",
        "  \"mightn't\": \"might not\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"needn't\": \"need not\",\n",
        "  \"mustn't\": \"must not\", \"shan't\": \"shall not\", \"she'd\": \"she would\", \"she'll\": \"she will\",\n",
        "  \"she's\": \"she is\", \"shouldn't\": \"should not\", \"should've\": \"should have\", \"that's\": \"that is\",\n",
        "  \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\", \"weren't\": \"were not\",\n",
        "  \"we've\": \"we have\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\", \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n",
        "  \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\", \"who'll\": \"who will\",\n",
        "  \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\",\n",
        "  \"you're\": \"you are\", \"you've\": \"you have\", \"wasn't\": \"was not\", \"we'll\": \" will\",\n",
        "  \"didn't\": \"did not\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all're\": \"you all are\"\n",
        "}\n",
        "\n",
        "def data_cleaning(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '', text) # clean url\n",
        "  text = re.sub(r'#(\\w+)', '', text)   # clean hashtags\n",
        "  text = re.sub(r'@(\\w+)', '', text)   # clean @s\n",
        "  text = re.sub(r'<[^>]+>', '', text)  # clean tags\n",
        "  text = re.sub(r'\\d+', '', text)      # clean digits\n",
        "  text = re.sub(r'’', '\\'', text)      # replace ’ with '\n",
        "  text = re.sub(r's\\'', '', text)      # clean s'\n",
        "  text = re.sub(r'[£₹$€₩]', ' ', text) # clean currency symbols\n",
        "  text = re.sub(r'[δ∫βωδσ∈∆≡απθ+*-=°^×√÷]', ' ', text) # clean math symbols\n",
        "  text = re.sub(r'[/(),!@\"“”?.%_&#:;><{}~\\[\\]|…]', ' ', text)   # clean punctuation\n",
        "  text = [contractions[word] if word in contractions else word for word in text.split()]  # change contractions to full forms\n",
        "  text = [WordNetLemmatizer().lemmatize(word) for word in text] # lemmatize\n",
        "  text = \" \".join(text)\n",
        "  text = re.sub(r'\\'s', '', text)      # clean 's\n",
        "  text = re.sub(r'\\'', '', text)       # clean '\n",
        "  return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSS9Br4Y6PBh"
      },
      "source": [
        "train['cleaned_question_text'] = train['question_text'].apply(data_cleaning)\n",
        "test['cleaned_question_text'] = test['question_text'].apply(data_cleaning)\n",
        "total_sentences = pd.concat([train['cleaned_question_text'], test['cleaned_question_text']], ignore_index = True)\n",
        "#train.loc[train['target'] == 1].sample(5)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erBnYIy-MH7i"
      },
      "source": [
        "def cloud(text, title, size = (10, 7)):\n",
        "  words_list = text.unique().tolist()\n",
        "  words = ' '.join(words_list)\n",
        "  wordcloud = WordCloud(width = 800, height = 400, collocations = False).generate(words)\n",
        "    \n",
        "  # Output Visualization\n",
        "  fig = plt.figure(figsize = size, dpi = 80, facecolor='k',edgecolor='k')\n",
        "  plt.imshow(wordcloud,interpolation = 'bilinear')\n",
        "  plt.axis('off')\n",
        "  plt.title(title, fontsize = 25,color = 'w')\n",
        "  plt.tight_layout(pad = 0)\n",
        "  plt.show()\n",
        "    \n",
        "#cloud(train[train['target'] == 0]['question_text_cleaned'], 'Cleaned Sincere questions')\n",
        "#cloud(train[train['target'] == 1]['question_text_cleaned'], 'Cleaned Insincere questions')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx4O3MNw4SMe"
      },
      "source": [
        "Reference: https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAByJTQyMQbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34104214-c3f2-4cb9-c005-2d34453c6652"
      },
      "source": [
        "def load_embeddings(file):\n",
        "  def get_coefs(word, *arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "    \n",
        "  if file == \"./GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\" :\n",
        "    embeddings_index = KeyedVectors.load_word2vec_format(file, binary = True)\n",
        "  elif file == \"../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\" :\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o) > 100)\n",
        "  else:\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
        "        \n",
        "  return embeddings_index\n",
        "\n",
        "paragram = \"./paragram_300_sl999/paragram_300_sl999.txt\"\n",
        "glove = \"./glove.840B.300d/glove.840B.300d.txt\"\n",
        "wiki_news = \"./wiki-news-300d-1M/wiki-news-300d-1M.vec\"\n",
        "google_news = \"./GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\n",
        "embedding_choice = paragram\n",
        "\n",
        "def pick_embedding(embedding):\n",
        "  print(\"Extracting \" + embedding + \" embedding\")\n",
        "  return load_embeddings(embedding)\n",
        "\n",
        "embedding_method = pick_embedding(embedding_choice)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./paragram_300_sl999/paragram_300_sl999.txt embedding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlEL3_70MS1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870a6dd7-9311-4a7c-dcb6-358c865fa4f3"
      },
      "source": [
        "def vocabulary_builder(corpus):\n",
        "  vocabulary = {}\n",
        "  for text in corpus:\n",
        "    for word in text.split():\n",
        "      try:\n",
        "        vocabulary[word] += 1\n",
        "      except KeyError:\n",
        "        vocabulary[word] = 1\n",
        "  return vocabulary\n",
        "\n",
        "def check_coverage(vocab, embeddings_index):\n",
        "  known_words = {}\n",
        "  unknown_words = {}\n",
        "  nb_known_words = 0\n",
        "  nb_unknown_words = 0\n",
        "  for word in vocab.keys():\n",
        "    try:\n",
        "      known_words[word] = embeddings_index[word]\n",
        "      nb_known_words += vocab[word]\n",
        "    except:\n",
        "      unknown_words[word] = vocab[word]\n",
        "      nb_unknown_words += vocab[word]\n",
        "      pass\n",
        "\n",
        "  print('Found embeddings for {:.2%} of vocabulary set'.format(len(known_words) / len(vocab)))\n",
        "  print('Found embeddings for {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
        "  unknown_words = sorted(unknown_words.items(), key = operator.itemgetter(1))[::-1]\n",
        "  return unknown_words\n",
        "\n",
        "def get_word_index(vocabulary):\n",
        "  word_index = dict((w, i + 1) for i, w in enumerate(vocabulary.keys()))\n",
        "  return word_index\n",
        "\n",
        "def oov_check():\n",
        "  print(embedding_choice)\n",
        "  check_coverage(vocabulary_set, embedding_method)\n",
        "  \n",
        "vocabulary_set = vocabulary_builder(total_sentences)\n",
        "vocabulary_size = len(vocabulary_set) + 1\n",
        "word_index = get_word_index(vocabulary_set)\n",
        "oov_check()\n",
        "max_length = 250\n",
        "#max_length = max([len(x) for x in total_sentences]) ~700"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./paragram_300_sl999/paragram_300_sl999.txt\n",
            "Found embeddings for 69.46% of vocabulary set\n",
            "Found embeddings for 99.46% of all text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmY1sUUXMasY"
      },
      "source": [
        "train_set, val_set = train_test_split(train, test_size = 0.2, random_state = 11)\n",
        "train_set_Y, val_set_Y = np.array(train_set['target']), np.array(val_set['target'])\n",
        "\n",
        "def use_encode():\n",
        "  def fit_one_hot(word_index, corpus):\n",
        "    sent = []\n",
        "    for text in corpus:\n",
        "      my_list = []\n",
        "      for word in text.split():\n",
        "        try:\n",
        "          my_list.append(word_index[word])\n",
        "        except KeyError:\n",
        "          my_list.append(0)\n",
        "      sent.append(my_list)\n",
        "    return sent\n",
        "  \n",
        "  training_sequence, validation_sequence, testing_sequence = train_set['cleaned_question_text'], val_set['cleaned_question_text'], test['cleaned_question_text']\n",
        "  encode_train_set, encode_val_set, encode_test_set = fit_one_hot(word_index, training_sequence), fit_one_hot(word_index, validation_sequence), fit_one_hot(word_index, testing_sequence)\n",
        "\n",
        "  x = pad_sequences(encode_train_set, maxlen = max_length, padding = 'post', truncating = 'post')\n",
        "  y = pad_sequences(encode_val_set, maxlen = max_length, padding = 'post', truncating = 'post')\n",
        "  z = pad_sequences(encode_test_set, maxlen = max_length, padding = 'post', truncating = 'post')\n",
        "  \n",
        "  return x, y, z\n",
        "\n",
        "def use_tokenize():\n",
        "  tokenizer = Tokenizer(num_words = vocabulary_size, oov_token = \"<OOV>\")\n",
        "  tokenizer.fit_on_texts(total_sentences)\n",
        "\n",
        "  training_sequences = tokenizer.texts_to_sequences(train_set['cleaned_question_text'])\n",
        "  validation_sequences = tokenizer.texts_to_sequences(val_set['cleaned_question_text'])\n",
        "  testing_sequences = tokenizer.texts_to_sequences(test['cleaned_question_text'])\n",
        "\n",
        "  z = pad_sequences(training_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n",
        "  y = pad_sequences(validation_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n",
        "  z = pad_sequences(testing_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n",
        "\n",
        "  return x, y, z\n",
        "\n",
        "training_padded, validation_padded, testing_padded = use_encode()\n",
        "#training_padded, validation_padded, testing_padded = use_tokenize()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOUDhugSMf70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73652342-6088-42c3-bc48-885ad4fac91d"
      },
      "source": [
        "def embedding_matrices(embed_type):\n",
        "  count = 0\n",
        "  embedding_matrix = np.zeros((vocabulary_size, 300))\n",
        "  for word,i in word_index.items():\n",
        "    try:\n",
        "      vec = embed_type[word]\n",
        "      embedding_matrix[i] = vec\n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      continue\n",
        "  return embedding_matrix, count\n",
        "\n",
        "embedding_matrix, count = embedding_matrices(embedding_method)\n",
        "print(\"Number of Out Of Vocabulary - OOVs: \", count)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Out Of Vocabulary - OOVs:  60359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tRP07n6MjzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58fdb7b-59bb-424a-c54e-86c1f111a84a"
      },
      "source": [
        "BATCH_SIZE = 2048\n",
        "EPOCHS = 10\n",
        "inputs = Input(shape = (max_length,))\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience = 2)\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.embed = Embedding(vocabulary_size, 300, weights = [embedding_matrix], trainable = False)\n",
        "    self.rnn = Sequential([\n",
        "        Bidirectional(LSTM(128, return_sequences = True)),\n",
        "        Bidirectional(LSTM(128, return_sequences = True)),\n",
        "        Conv1D(128, 3, activation = \"relu\"),\n",
        "        GlobalMaxPool1D(),\n",
        "        Dense(64, activation = \"relu\"),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation = \"relu\"),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation = \"sigmoid\")\n",
        "    ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embed(inputs)\n",
        "    x = self.rnn(x)\n",
        "    return x\n",
        "\n",
        "model = MyModel()\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics = ['binary_accuracy'])\n",
        "model.fit(x = training_padded, y = train_set_Y, batch_size = BATCH_SIZE, epochs = EPOCHS, callbacks = callback, validation_data = (validation_padded, val_set_Y))\n",
        "predicted = model.predict(testing_padded, batch_size = 512)\n",
        "test['prediction'] = predicted"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "511/511 [==============================] - 768s 1s/step - loss: 0.1335 - binary_accuracy: 0.9502 - val_loss: 0.1119 - val_binary_accuracy: 0.9554\n",
            "Epoch 2/3\n",
            "511/511 [==============================] - 725s 1s/step - loss: 0.1095 - binary_accuracy: 0.9574 - val_loss: 0.1054 - val_binary_accuracy: 0.9582\n",
            "Epoch 3/3\n",
            "511/511 [==============================] - 725s 1s/step - loss: 0.1018 - binary_accuracy: 0.9600 - val_loss: 0.1056 - val_binary_accuracy: 0.9583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2JkqRd7nshl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2b9b0c-ba94-45a7-c1b8-3eb870983aa4"
      },
      "source": [
        "test[\"#\"] = [0 if x < 0.5 else 1 for x in test[\"prediction\"]]\n",
        "percentage_insincere = round((test[\"#\"].values == 1).sum() / (test.shape[0]) * 100, 2)\n",
        "print(\"Percentage of insincere questions in the test dataset: {}% \".format(percentage_insincere))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of insincere questions in the test dataset: 4.27% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXLK5UlHZxJK"
      },
      "source": [
        "submission = test.drop([\"prediction\", \"cleaned_question_text\"], axis=1)\n",
        "submission.to_csv(root + \"/submission.csv\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "SHkwNwDuapRZ",
        "outputId": "2eed496a-82f2-489c-8ccb-9dd0b3261864"
      },
      "source": [
        "#check\n",
        "subm = pd.read_csv(root + \"/submission.csv\")\n",
        "subm[[\"question_text\", \"#\"]].loc[subm[\"#\"] == 1].sample(10)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_text</th>\n",
              "      <th>#</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>266920</th>\n",
              "      <td>Why do Indians stalk girls on Quora?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11570</th>\n",
              "      <td>Should I name my girl Pussy?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84321</th>\n",
              "      <td>Are there any liberals who actually call thems...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280696</th>\n",
              "      <td>Is it true all Quora moderators make minimum w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363465</th>\n",
              "      <td>Why aren’t my African American sisters more up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355675</th>\n",
              "      <td>Do Muslim refugees have a higher percentage of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112695</th>\n",
              "      <td>Is China really bad as Chinese people think?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218631</th>\n",
              "      <td>People don't care about people lying or morals...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312581</th>\n",
              "      <td>Why are Liberals a bunch of cowards?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278769</th>\n",
              "      <td>Given the religious bigotry, misogyny, scorn o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question_text  #\n",
              "266920               Why do Indians stalk girls on Quora?  1\n",
              "11570                        Should I name my girl Pussy?  1\n",
              "84321   Are there any liberals who actually call thems...  1\n",
              "280696  Is it true all Quora moderators make minimum w...  1\n",
              "363465  Why aren’t my African American sisters more up...  1\n",
              "355675  Do Muslim refugees have a higher percentage of...  1\n",
              "112695       Is China really bad as Chinese people think?  1\n",
              "218631  People don't care about people lying or morals...  1\n",
              "312581               Why are Liberals a bunch of cowards?  1\n",
              "278769  Given the religious bigotry, misogyny, scorn o...  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}